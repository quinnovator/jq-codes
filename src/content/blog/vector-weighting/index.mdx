---
title: 'Vector Weighting for Personalization'
description: A deep dive into vector-based recommendation systems and dynamic preference weighting.
date: 2024-12-09
tags: ['personalization', 'vectors', 'machine-learning']
authors: ['jack']
---

import { PreferenceDemo } from './preference-demo';
import { EventTable } from './event-table';
import { WeightedDemo } from './weighted-demo';
import { QueryDemo } from './query-demo';
import { BasicQueryDemo } from './basic-query-demo';

# Understanding Vector-Based Recommendation Systems

One of the best ways that we have to personalize content is by using vector embeddings. When we have a bunch of data that we capture about the user, but aren't able to match the data with a direct text search, we can embed the meaning of the data into a vector of $n$ dimensions.

What does this mean? We can use soemthing called an embedding model to represent text as a series of numbers. Each entity that is in our datastore will have a similar series of numbers. The number of numbers that we use to represent the meaning of the text is called the dimensionality of the embedding.

There is a trade off when it comes to dimensionality. The more dimensions we use, the more accurate our embedding will be. However, the more dimensions we use, the more computationally expensive it will be to search for similar entities.

Once we represent our data this way, we can use a technique called vector search to find entries that are similar in meaning to a given query.

## How Vector Embeddings Work

Here's an example of how an event might be represented as a vector:

```typescript
const event =
  'A scenic park offering walking trails, a playground, and a picnic area.';

const embedding = await getEmbedding(event); // returns a vector of 300 dimensions

console.log(embedding); // [0.2, 0.8, -0.3, ...]
```

The position of these vectors in a high-dimensional space shows us semantic relationships - similar items will be closer together. We use something called cosine similarity to measure the similarity between two vectors in most cases. There are other ways to measure similarity, but cosine similarity is a good default.

```math
similarity(a, b) = \frac{a \cdot b}{||a|| \cdot ||b||}
```

## Demonstrating Vector Similarity

I'll share a basic demonstration of how vector similarity works by embedding a list of events into a vector space. We'll use these embeddings to find similar events to a given query.

<EventTable client:only="react" />

The description of each event is embedded into a vector space. This is occuring within your browser behind the scenes, using `transformers.js` and `rxdb`.

## Finding Similar Events

Now that we have a list of events embedded into a vector space, we can use cosine similarity to find similar events to a given query.

Try searching for an event below without using the name of the event, to see how well the system can understand meaning.

<BasicQueryDemo client:only="react" />
